{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://www.kaggle.com/mbrown89/boost-your-score-guaranteed-leaderboard-probing/data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "class LeaderBoardProbing:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.test=pd.read_csv(os.path.join(DATA_DIR,'test.csv'))\n",
    "        sales=pd.read_csv(os.path.join(DATA_DIR, 'sales_train.csv'))\n",
    "        # some routine data cleaning code\n",
    "        shop_id_map={0:57, 1:58, 10:11}\n",
    "        sales['shop_id']=sales['shop_id'].apply(lambda x: shop_id_map.get(x, x))\n",
    "        self.test['shop_id']=self.test['shop_id'].apply(lambda x: shop_id_map.get(x, x))\n",
    "\n",
    "        pairs={ (a, b) for a, b in zip(sales.shop_id, sales.item_id) }\n",
    "        items={ a for a in sales.item_id }\n",
    "        self.test['date_block_num']=34\n",
    "        self.test['test_group']=[ 2 if (a,b) in pairs else (1 if b in items else 0) for a,b in zip(self.test.shop_id, self.test.item_id)]\n",
    "        self.test.sort_values('ID', inplace=True)\n",
    "        self.test.to_csv('new_test.csv.gz', index=False)\n",
    "\n",
    "        self.test['item_cnt_month']=0.0\n",
    "        self.n=len(self.test)\n",
    "        self.n0=sum(self.test.test_group==0)\n",
    "        self.n1=sum(self.test.test_group==1)\n",
    "        self.n2=sum(self.test.test_group==2)\n",
    "\n",
    "    def probe_mean(self):\n",
    "        \"\"\"Generate 4 LeaderBoardProbing files, set target to 0 for all three test groups,\n",
    "        then set target to 1 for only one test group at a time.\n",
    "        Manually submit the files to obtain public leaderboard scores.\n",
    "        Then feed the scores to estimate_mean() to obtain mean values for all groups\n",
    "        and store those means in group_mean()\n",
    "        \"\"\"\n",
    "        os.makedirs('leak', exist_ok=True)\n",
    "        self.save(self.test, 'leak/Probe000.csv')\n",
    "\n",
    "        tmp=self.test.copy()\n",
    "        tmp.loc[tmp.test_group==2, 'item_cnt_month']=1.0\n",
    "        self.save(tmp, 'leak/Probe001.csv')\n",
    "\n",
    "        tmp=self.test.copy()\n",
    "        tmp.loc[tmp.test_group==1, 'item_cnt_month']=1.0\n",
    "        self.save(tmp, 'leak/Probe010.csv')\n",
    "\n",
    "        tmp=self.test.copy()\n",
    "        tmp.loc[tmp.test_group==0, 'item_cnt_month']=1.0\n",
    "        self.save(tmp, 'leak/Probe100.csv')\n",
    "\n",
    "    def estimate_mean(self, rmse000, rmse100, rmse010, rmse001):\n",
    "        \"\"\"Obtain public scores for Probe000, Probe100, Probe010, Probe001\n",
    "        Public,Private\n",
    "        Probe000,1.250111,1.236582\n",
    "        Probe100,1.23528,1.221182\n",
    "        Probe010,1.38637,1.373707\n",
    "        Probe001,1.29326,1.279869\n",
    "        \"\"\"\n",
    "\n",
    "        def calc(rmse000, n, rmse_i, ni):\n",
    "            u=(1-(rmse_i**2-rmse000**2)*n/ni)/2\n",
    "            return u\n",
    "\n",
    "        u0=calc(rmse000, self.n, rmse100, self.n0)\n",
    "        u1=calc(rmse000, self.n, rmse010, self.n1)\n",
    "        u2=calc(rmse000, self.n, rmse001, self.n2)\n",
    "        u=(self.n0*u0+self.n1*u1+self.n2*u2)/self.n\n",
    "        return(u0, u1, u2, u)\n",
    "\n",
    "    def true_means(self):\n",
    "        # computed by leader board probing\n",
    "        # u0, u1, u2 and overall mean u\n",
    "        # Kaggle public scores and Coursera scores slightly differ\n",
    "        # Kaggle scores\n",
    "        #return [0.7590957299173547, 0.060230457160248385, 0.39458181098366407, 0.2839717256500001]\n",
    "        # use Coursera scores here\n",
    "        return [0.758939742420249, 0.0601995732152425, 0.3945593622881204, 0.28393632703149974]\n",
    "\n",
    "    def mean_scale(self, filename):\n",
    "        \"\"\"Compare the mean of each test group to their true public leaderboard means\n",
    "        shift the prediction so that the means match\n",
    "        filename: your submission csv file name\n",
    "        \"\"\"\n",
    "        df=pd.read_csv(filename)\n",
    "        df.sort_values('ID', ascending=True, inplace=True)\n",
    "        mask0=self.test.test_group==0\n",
    "        mask1=self.test.test_group==1\n",
    "        mask2=self.test.test_group==2\n",
    "        U=self.true_means()\n",
    "        print(\"Group0: predict mean=\", df[ mask0 ].item_cnt_month.mean(), \"true mean=\", U[0])\n",
    "        print(\"Group1: predict mean=\", df[ mask1 ].item_cnt_month.mean(), \"true mean=\", U[1])\n",
    "        print(\"Group2: predict mean=\", df[ mask2 ].item_cnt_month.mean(), \"true mean=\", U[2])\n",
    "        change=999\n",
    "        previous=df.item_cnt_month.values.copy()\n",
    "        i=1\n",
    "        while change>1e-6:\n",
    "            df.loc[mask0, 'item_cnt_month']+=U[0]-df[ mask0 ].item_cnt_month.mean()\n",
    "            df.loc[mask1, 'item_cnt_month']+=U[1]-df[ mask1 ].item_cnt_month.mean()\n",
    "            df.loc[mask2, 'item_cnt_month']+=U[2]-df[ mask2 ].item_cnt_month.mean()\n",
    "            df['item_cnt_month']=df['item_cnt_month'].clip(0,20)\n",
    "            change=np.sum(np.abs(df.item_cnt_month.values - previous))\n",
    "            previous=df.item_cnt_month.values.copy()\n",
    "            print(\">loop\", i, \"change:\", change)\n",
    "            i+=1\n",
    "        self.save(df, filename.replace('.csv', '_mean.csv'))\n",
    "\n",
    "    def variance_scale(self, filename, rmse, rmse000=1.250111):\n",
    "        \"\"\"\n",
    "        filename: your submission csv file name\n",
    "        rmse: your public leaderboard score\n",
    "        \"\"\"\n",
    "        df=pd.read_csv(filename)\n",
    "        df.sort_values('ID', ascending=True, inplace=True)\n",
    "        n=df.shape[0]\n",
    "        u=self.true_means()[-1]\n",
    "        Yp=df.item_cnt_month.values\n",
    "        YpYp=np.sum(Yp*Yp)\n",
    "        YYp=n*(rmse000**2-rmse**2)/2+YpYp/2\n",
    "        lambda_ = (YYp-u*u*n)/(YpYp-u*u*n)\n",
    "        print(\">>>>>multipler lambda=\", lambda_)\n",
    "        df['item_cnt_month']=(Yp-u)*lambda_+u\n",
    "        filename2=filename.replace('.csv', '_lambda.csv')\n",
    "        self.save(df, filename2)\n",
    "        self.mean_scale(filename2)\n",
    "\n",
    "    def save(self, df, filename):\n",
    "        \"\"\"Produce LeaderBoardProbing file based on dataframe\"\"\"\n",
    "        df = df[['ID','item_cnt_month']].copy()\n",
    "        df.sort_values(['ID'], ascending=True, inplace=True)\n",
    "        df['item_cnt_month']=df['item_cnt_month'].apply(lambda x: \"%.5f\" % x)\n",
    "        if np.isnan(df.item_cnt_month.isnull().sum()):\n",
    "            print(\"ERROR>>>>> There should be no nan entry in the LeaderBoardProbing file!\")\n",
    "        print(\"Save LeaderBoardProbing to file:\", filename)\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    def flip_signs(self, filename):\n",
    "        \"\"\"\n",
    "        Produce LeaderBoardProbing file, flip the sign of prediction for each of the three groups\n",
    "        filename: your submission csv file name\n",
    "        output:\n",
    "            three new submission files with suffix _mpp.csv, _pmp.csv, _ppm.csv\n",
    "            notation in the notebook\n",
    "            m: minus, p: plus\n",
    "            mpp is -++, pmp is +-+, ppm is ++-\n",
    "\n",
    "        You need to submit these three files to obtain\n",
    "            rmse_mpp, rmse_pmp, rmse_ppm\n",
    "        Then you call\n",
    "            LeaderBoardProbing.variance_scale_v2(filename, rmse_mpp, rmse_pmp, rmse_ppm, rmse)\n",
    "            Note: rmse is the original rmse score obtained by your filename\n",
    "        \"\"\"\n",
    "        df=pd.read_csv(filename)\n",
    "        df.sort_values(['ID'], ascending=True, inplace=True)\n",
    "        mask0=self.test.test_group==0\n",
    "        mask1=self.test.test_group==1\n",
    "        mask2=self.test.test_group==2\n",
    "        tmp=df.copy()\n",
    "        tmp.loc[mask0, 'item_cnt_month']=-tmp[ mask0 ].item_cnt_month\n",
    "        self.save(tmp, filename.replace('.csv', '_mpp.csv'))\n",
    "        tmp=df.copy()\n",
    "        tmp.loc[mask1, 'item_cnt_month']=-tmp[ mask1 ].item_cnt_month\n",
    "        self.save(tmp, filename.replace('.csv', '_pmp.csv'))\n",
    "        tmp=df.copy()\n",
    "        tmp.loc[mask2, 'item_cnt_month']=-tmp[ mask2 ].item_cnt_month\n",
    "        self.save(tmp, filename.replace('.csv', '_ppm.csv'))\n",
    "\n",
    "    def variance_scale_v2(self, filename, rmse_mpp, rmse_pmp, rmse_ppm, rmse):\n",
    "        \"\"\"\n",
    "        filename: your submission csv file name\n",
    "        You must use LeaderBoardProbing.flip_signs(filename)\n",
    "            to generate three additional submission files, obtain their public scores\n",
    "            and feed those scores as parameters\n",
    "        Scores: rmse-++, rmse+-+, rmse++-, rmse+++\n",
    "\n",
    "        output:\n",
    "            New scaled submission file\n",
    "        \"\"\"\n",
    "        df=pd.read_csv(filename)\n",
    "        df.sort_values(['ID'], ascending=True, inplace=True)\n",
    "        mask0=self.test.test_group==0\n",
    "        mask1=self.test.test_group==1\n",
    "        mask2=self.test.test_group==2\n",
    "        n=len(df)\n",
    "        n0=sum(mask0)\n",
    "        n1=sum(mask1)\n",
    "        n2=sum(mask2)\n",
    "        YYp0=n/4*(rmse_mpp**2-rmse**2)\n",
    "        YYp1=n/4*(rmse_pmp**2-rmse**2)\n",
    "        YYp2=n/4*(rmse_ppm**2-rmse**2)\n",
    "        U=self.true_means()\n",
    "        Yp0=df.loc[mask0, 'item_cnt_month'].values\n",
    "        Yp1=df.loc[mask1, 'item_cnt_month'].values\n",
    "        Yp2=df.loc[mask2, 'item_cnt_month'].values\n",
    "        lambda0=(YYp0-U[0]**2*n0)/(np.sum(Yp0*Yp0)-U[0]**2*n0)\n",
    "        lambda1=(YYp1-U[1]**2*n1)/(np.sum(Yp1*Yp1)-U[1]**2*n1)\n",
    "        lambda2=(YYp2-U[2]**2*n2)/(np.sum(Yp2*Yp2)-U[2]**2*n2)\n",
    "        print(\"Labmda: \", lambda0, lambda1, lambda2)\n",
    "        df.loc[mask0, 'item_cnt_month']=U[0]+lambda0*(df[ mask0 ].item_cnt_month-U[0])\n",
    "        df.loc[mask1, 'item_cnt_month']=U[1]+lambda1*(df[ mask1 ].item_cnt_month-U[1])\n",
    "        df.loc[mask2, 'item_cnt_month']=U[2]+lambda2*(df[ mask2 ].item_cnt_month-U[2])\n",
    "        df['item_cnt_month']=df['item_cnt_month'].clip(0,20)\n",
    "        fn=filename.replace('.csv', '_labmdaV2.csv')\n",
    "        self.save(df, fn)\n",
    "        self.mean_scale(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group0: predict mean= 0.8240911625616667 true mean= 0.758939742420249\n",
      "Group1: predict mean= 0.07654534640393983 true mean= 0.0601995732152425\n",
      "Group2: predict mean= 0.372822242516336 true mean= 0.3945593622881204\n",
      ">loop 1 change: 4309.989019927566\n",
      ">loop 2 change: 397.4842882943934\n",
      ">loop 3 change: 100.70253869346402\n",
      ">loop 4 change: 27.459713901931405\n",
      ">loop 5 change: 7.672664585976546\n",
      ">loop 6 change: 2.1656835113777615\n",
      ">loop 7 change: 0.6147213847326545\n",
      ">loop 8 change: 0.17529071618127515\n",
      ">loop 9 change: 0.05016009276223721\n",
      ">loop 10 change: 0.014397686912328733\n",
      ">loop 11 change: 0.004143973375257848\n",
      ">loop 12 change: 0.0011956852826626035\n",
      ">loop 13 change: 0.00034576781783551835\n",
      ">loop 14 change: 0.00010018874802451838\n",
      ">loop 15 change: 2.9082189744900067e-05\n",
      ">loop 16 change: 8.455094461345958e-06\n",
      ">loop 17 change: 2.4616113630804293e-06\n",
      ">loop 18 change: 7.176337196712623e-07\n",
      "Save LeaderBoardProbing to file: probing/submission-final_mean.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "FILE_DIR = 'probing'\n",
    "lbp = LeaderBoardProbing()\n",
    "lbp.mean_scale(os.path.join(FILE_DIR, 'submission-final.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original->Your public and private LB scores are: 0.828486 and 0.823450.\n",
    "\n",
    "after the mean scaling->Your public and private LB scores are: 0.827726 and 0.822908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>multipler lambda= 1.0068056484384023\n",
      "Save LeaderBoardProbing to file: probing/submission-final_mean_lambda.csv\n",
      "Group0: predict mean= 0.7621731122916819 true mean= 0.758939742420249\n",
      "Group1: predict mean= 0.05867760799240407 true mean= 0.0601995732152425\n",
      "Group2: predict mean= 0.39531219653542643 true mean= 0.3945593622881204\n",
      ">loop 1 change: 278.36865601168125\n",
      ">loop 2 change: 18.63568117389056\n",
      ">loop 3 change: 4.97555400486474\n",
      ">loop 4 change: 1.391606977887781\n",
      ">loop 5 change: 0.3912674940753036\n",
      ">loop 6 change: 0.11042405033764335\n",
      ">loop 7 change: 0.031276808593900945\n",
      ">loop 8 change: 0.00889061440673175\n",
      ">loop 9 change: 0.002535900795186869\n",
      ">loop 10 change: 0.0007259552492794308\n",
      ">loop 11 change: 0.00020852648642807892\n",
      ">loop 12 change: 6.00897285342461e-05\n",
      ">loop 13 change: 1.7368634887895973e-05\n",
      ">loop 14 change: 5.034828301095362e-06\n",
      ">loop 15 change: 1.4635805384022316e-06\n",
      ">loop 16 change: 4.2642215007659967e-07\n",
      "Save LeaderBoardProbing to file: probing/submission-final_mean_lambda_mean.csv\n"
     ]
    }
   ],
   "source": [
    "lbp.variance_scale(os.path.join(FILE_DIR, 'submission-final_mean.csv'), 0.827726)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your public and private LB scores are: 0.827716 and 0.822888."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save LeaderBoardProbing to file: probing/submission-final_mean_mpp.csv\n",
      "Save LeaderBoardProbing to file: probing/submission-final_mean_pmp.csv\n",
      "Save LeaderBoardProbing to file: probing/submission-final_mean_ppm.csv\n"
     ]
    }
   ],
   "source": [
    "lbp.flip_signs(os.path.join(FILE_DIR, 'submission-final_mean.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission,Public,Private\n",
    "\n",
    "submission-final_mean_mpp.csv, Your public and private LB scores are: 1.457908 and 1.453490.\n",
    "\n",
    "submission-final_mean_pmp.csv, Your public and private LB scores are: 0.852850 and 0.844612.\n",
    "\n",
    "submission-final_mean_ppm.csv, Your public and private LB scores are: 1.655513 and 1.617827."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labmda:  0.8903581511236648 1.1060851394194742 1.1486633830454325\n",
      "Save LeaderBoardProbing to file: probing/submission-final_mean_labmdaV2.csv\n",
      "Group0: predict mean= 0.7589393572085058 true mean= 0.758939742420249\n",
      "Group1: predict mean= 0.06227713087114098 true mean= 0.0601995732152425\n",
      "Group2: predict mean= 0.39425648060930363 true mean= 0.3945593622881204\n",
      ">loop 1 change: 147.64797089348082\n",
      ">loop 2 change: 41.37019674807945\n",
      ">loop 3 change: 15.976880555438102\n",
      ">loop 4 change: 6.317577526411823\n",
      ">loop 5 change: 2.5214492717507\n",
      ">loop 6 change: 1.0094791939310601\n",
      ">loop 7 change: 0.40468010329338117\n",
      ">loop 8 change: 0.16231810535779084\n",
      ">loop 9 change: 0.06514581118636899\n",
      ">loop 10 change: 0.026146046450402347\n",
      ">loop 11 change: 0.010493625517551147\n",
      ">loop 12 change: 0.004211580440570115\n",
      ">loop 13 change: 0.0016903033188069433\n",
      ">loop 14 change: 0.000678397482154619\n",
      ">loop 15 change: 0.0002722725126705766\n",
      ">loop 16 change: 0.00010927574802814533\n",
      ">loop 17 change: 4.385748717680715e-05\n",
      ">loop 18 change: 1.7602113070377956e-05\n",
      ">loop 19 change: 7.064575323401101e-06\n",
      ">loop 20 change: 2.835370640538737e-06\n",
      ">loop 21 change: 1.1380545643646656e-06\n",
      ">loop 22 change: 4.5680447599560914e-07\n",
      "Save LeaderBoardProbing to file: probing/submission-final_mean_labmdaV2_mean.csv\n"
     ]
    }
   ],
   "source": [
    "lbp.variance_scale_v2(os.path.join(FILE_DIR, 'submission-final_mean.csv'), 1.457908, 0.852850, 1.655513, 0.827726)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your public and private LB scores are: 0.822393 and 0.818753."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- 000: Your public and private LB scores are: 1.250111 and 1.236582\n",
    "\n",
    "001: Your public and private LB scores are: 1.293260 and 1.279869\n",
    "\n",
    "010: Your public and private LB scores are: 1.386370 and 1.373707.\n",
    "\n",
    "100: Your public and private LB scores are: 1.235280 and 1.221182. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
