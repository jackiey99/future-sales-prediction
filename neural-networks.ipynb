{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/yashu-seth/pytorch-tabular/blob/master/pytorch_tabular.py\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, label, cat_cols=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pandas data frame\n",
    "        The data frame object for the input data. It must\n",
    "        contain all the continuous, categorical \n",
    "        label: the output labels\n",
    "        cat_cols: List of strings\n",
    "        The names of the categorical columns in the data.\n",
    "        These columns will be passed through the embedding\n",
    "        layers in the model. These columns must be\n",
    "        label encoded beforehand. \n",
    "        output_col: string\n",
    "        The name of the output variable column in the data\n",
    "        provided.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = data.shape[0]\n",
    "        self.y = label.astype(np.float32).values.reshape(-1, 1)\n",
    "        \n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else []\n",
    "        self.cont_cols = [\n",
    "            col for col in data.columns if col not in self.cat_cols\n",
    "        ]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].values\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].astype(np.int64).values\n",
    "        else:\n",
    "            self.cat_X = np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dims,\n",
    "        no_of_cont,\n",
    "        lin_layer_sizes,\n",
    "        output_size,\n",
    "        emb_dropout,\n",
    "        lin_layer_dropouts,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        emb_dims: List of two element tuples\n",
    "        This list will contain a two element tuple for each\n",
    "        categorical feature. The first element of a tuple will\n",
    "        denote the number of unique values of the categorical\n",
    "        feature. The second element will denote the embedding\n",
    "        dimension to be used for that feature.\n",
    "        no_of_cont: Integer\n",
    "        The number of continuous features in the data.\n",
    "        lin_layer_sizes: List of integers.\n",
    "        The size of each linear layer. The length will be equal\n",
    "        to the total number\n",
    "        of linear layers in the network.\n",
    "        output_size: Integer\n",
    "        The size of the final output.\n",
    "        emb_dropout: Float\n",
    "        The dropout to be used after the embedding layers.\n",
    "        lin_layer_dropouts: List of floats\n",
    "        The dropouts to be used after each linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n",
    "\n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.no_of_cont = no_of_cont\n",
    "\n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(\n",
    "            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n",
    "        )\n",
    "\n",
    "        self.lin_layers = nn.ModuleList(\n",
    "            [first_lin_layer]\n",
    "            + [\n",
    "                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "                for i in range(len(lin_layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        # Batch Norm Layers\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList(\n",
    "            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n",
    "        )\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList(\n",
    "            [nn.Dropout(size) for size in lin_layer_dropouts]\n",
    "        )\n",
    "\n",
    "    def forward(self, cont_data, cat_data):\n",
    "        \n",
    "        if self.no_of_embs != 0:\n",
    "            x = [\n",
    "                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n",
    "            ]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout_layer(x)\n",
    "\n",
    "        if self.no_of_cont != 0:\n",
    "            normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "\n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, normalized_cont_data], 1)\n",
    "            else:\n",
    "                x = normalized_cont_data\n",
    "\n",
    "        for lin_layer, dropout_layer, bn_layer in zip(\n",
    "            self.lin_layers, self.droput_layers, self.bn_layers\n",
    "        ):\n",
    "\n",
    "            x = F.relu(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('data_feat.h5', 'table')\n",
    "\n",
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month', 'date_block_num'], axis=1).fillna(-1)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month', 'date_block_num'], axis=1).fillna(-1)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month', 'date_block_num'], axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.clip(0, 20)\n",
    "Y_valid = Y_valid.clip(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unseen shop_id in valid: 5413\n",
      "#unseen shop_id in test: 5100\n",
      "#unseen item_id in valid: 21076\n",
      "#unseen item_id in test: 27174\n",
      "#unseen shop_category in valid: 0\n",
      "#unseen shop_category in test: 0\n",
      "#unseen shop_city in valid: 0\n",
      "#unseen shop_city in test: 0\n",
      "#unseen item_category_id in valid: 0\n",
      "#unseen item_category_id in test: 42\n",
      "#unseen name2 in valid: 176\n",
      "#unseen name2 in test: 924\n",
      "#unseen name3 in valid: 1232\n",
      "#unseen name3 in test: 2310\n",
      "#unseen subtype_code in valid: 0\n",
      "#unseen subtype_code in test: 42\n",
      "#unseen type_code in valid: 0\n",
      "#unseen type_code in test: 0\n",
      "#unseen month in valid: 0\n",
      "#unseen month in test: 0\n",
      "#unseen days in valid: 0\n",
      "#unseen days in test: 0\n"
     ]
    }
   ],
   "source": [
    "categorical_features = ['shop_id', 'item_id', 'shop_category', 'shop_city', 'item_category_id', 'name2', 'name3', 'subtype_code', 'type_code', 'month', 'days']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for cat_col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train[cat_col])\n",
    "    le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    unseen = max(le_dict.values()) + 1\n",
    "    X_train[cat_col] = X_train[cat_col].apply(lambda x: le_dict.get(x, unseen))\n",
    "    X_valid[cat_col] = X_valid[cat_col].apply(lambda x: le_dict.get(x, unseen))\n",
    "    X_test[cat_col] = X_test[cat_col].apply(lambda x: le_dict.get(x, unseen))\n",
    "    print(\"#unseen {} in valid: {}\".format(cat_col, sum(X_valid[cat_col] == unseen)))\n",
    "    print(\"#unseen {} in test: {}\".format(cat_col, sum(X_test[cat_col] == unseen)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(data=X_train, label=Y_train, cat_cols=categorical_features)\n",
    "valid_dataset = TabularDataset(data=X_valid, label=Y_valid, cat_cols=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=1)\n",
    "valid_loader = DataLoader(valid_dataset, batchsize, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 20041, 6, 32, 82, 157, 1530, 65, 11, 13, 4]\n",
      "[(55, 28), (20041, 50), (6, 3), (32, 16), (82, 41), (157, 50), (1530, 50), (65, 33), (11, 6), (13, 7), (4, 2)]\n"
     ]
    }
   ],
   "source": [
    "cat_dims = [int(X_train[col].nunique()) + 1 for col in categorical_features]\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "\n",
    "print(cat_dims)\n",
    "print(emb_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = FeedForwardNN(emb_dims, no_of_cont=21, lin_layer_sizes=[50, 100],\n",
    "                          output_size=1, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.986337 \tValidation Loss: 0.946498\n",
      "3487.8065943717957\n",
      "Validation loss decreased (inf --> 0.946498).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.888039 \tValidation Loss: 1.058650\n",
      "3635.751012802124\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "no_of_epochs = 100\n",
    "\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "for epoch in range(no_of_epochs):\n",
    "    ts = time.time()\n",
    "    # monitor the training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ####\n",
    "    # train the model\n",
    "    ####\n",
    "    model.train()\n",
    "    for y, cont_x, cat_x in dataloader:\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(cont_x, cat_x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cat_x.size(0)\n",
    "          \n",
    "    model.eval()\n",
    "    for y, cont_x, cat_x in valid_loader:\n",
    "        output = model(cont_x, cat_x)\n",
    "        output = torch.clamp(output, min=0, max=20)\n",
    "        loss = criterion(output, y)\n",
    "        valid_loss += loss.item() * cat_x.size(0)\n",
    "    \n",
    "    train_loss = math.sqrt(train_loss/len(dataloader.sampler))\n",
    "    valid_loss = math.sqrt(valid_loss/len(valid_loader.sampler))\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    print(time.time() - ts)\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
