{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/yashu-seth/pytorch-tabular/blob/master/pytorch_tabular.py\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, cat_cols=None, output_col=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pandas data frame\n",
    "        The data frame object for the input data. It must\n",
    "        contain all the continuous, categorical and the\n",
    "        output columns to be used.\n",
    "        cat_cols: List of strings\n",
    "        The names of the categorical columns in the data.\n",
    "        These columns will be passed through the embedding\n",
    "        layers in the model. These columns must be\n",
    "        label encoded beforehand. \n",
    "        output_col: string\n",
    "        The name of the output variable column in the data\n",
    "        provided.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = data.shape[0]\n",
    "\n",
    "        if output_col:\n",
    "            self.y = data[output_col].values.reshape(-1, 1)\n",
    "        else:\n",
    "            self.y = np.zeros((self.n, 1))\n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else []\n",
    "        self.cont_cols = [\n",
    "            col for col in data.columns if col not in self.cat_cols + [output_col]\n",
    "        ]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].values\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].values\n",
    "        else:\n",
    "            self.cat_X = np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dims,\n",
    "        no_of_cont,\n",
    "        lin_layer_sizes,\n",
    "        output_size,\n",
    "        emb_dropout,\n",
    "        lin_layer_dropouts,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        emb_dims: List of two element tuples\n",
    "        This list will contain a two element tuple for each\n",
    "        categorical feature. The first element of a tuple will\n",
    "        denote the number of unique values of the categorical\n",
    "        feature. The second element will denote the embedding\n",
    "        dimension to be used for that feature.\n",
    "        no_of_cont: Integer\n",
    "        The number of continuous features in the data.\n",
    "        lin_layer_sizes: List of integers.\n",
    "        The size of each linear layer. The length will be equal\n",
    "        to the total number\n",
    "        of linear layers in the network.\n",
    "        output_size: Integer\n",
    "        The size of the final output.\n",
    "        emb_dropout: Float\n",
    "        The dropout to be used after the embedding layers.\n",
    "        lin_layer_dropouts: List of floats\n",
    "        The dropouts to be used after each linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n",
    "\n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.no_of_cont = no_of_cont\n",
    "\n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(\n",
    "            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n",
    "        )\n",
    "\n",
    "        self.lin_layers = nn.ModuleList(\n",
    "            [first_lin_layer]\n",
    "            + [\n",
    "                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "                for i in range(len(lin_layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        # Batch Norm Layers\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList(\n",
    "            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n",
    "        )\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList(\n",
    "            [nn.Dropout(size) for size in lin_layer_dropouts]\n",
    "        )\n",
    "\n",
    "    def forward(self, cont_data, cat_data):\n",
    "\n",
    "        if self.no_of_embs != 0:\n",
    "            x = [\n",
    "                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n",
    "            ]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout_layer(x)\n",
    "\n",
    "        if self.no_of_cont != 0:\n",
    "            normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "\n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, normalized_cont_data], 1)\n",
    "            else:\n",
    "                x = normalized_cont_data\n",
    "\n",
    "        for lin_layer, dropout_layer, bn_layer in zip(\n",
    "            self.lin_layers, self.droput_layers, self.bn_layers\n",
    "        ):\n",
    "\n",
    "            x = F.relu(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
