{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/yashu-seth/pytorch-tabular/blob/master/pytorch_tabular.py\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, label, cat_cols=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pandas data frame\n",
    "        The data frame object for the input data. It must\n",
    "        contain all the continuous, categorical \n",
    "        label: the output labels\n",
    "        cat_cols: List of strings\n",
    "        The names of the categorical columns in the data.\n",
    "        These columns will be passed through the embedding\n",
    "        layers in the model. These columns must be\n",
    "        label encoded beforehand. \n",
    "        output_col: string\n",
    "        The name of the output variable column in the data\n",
    "        provided.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = data.shape[0]\n",
    "        if label:\n",
    "            self.y = label.astype(np.float32).values.reshape(-1, 1)\n",
    "        else:\n",
    "            self.y = np.zeros((self.n, 1))\n",
    "        \n",
    "\n",
    "        self.cat_cols = cat_cols if cat_cols else []\n",
    "        self.cont_cols = [\n",
    "            col for col in data.columns if col not in self.cat_cols\n",
    "        ]\n",
    "\n",
    "        if self.cont_cols:\n",
    "            self.cont_X = data[self.cont_cols].values\n",
    "        else:\n",
    "            self.cont_X = np.zeros((self.n, 1))\n",
    "\n",
    "        if self.cat_cols:\n",
    "            self.cat_X = data[cat_cols].astype(np.int64).values\n",
    "        else:\n",
    "            self.cat_X = np.zeros((self.n, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dims,\n",
    "        no_of_cont,\n",
    "        lin_layer_sizes,\n",
    "        output_size,\n",
    "        emb_dropout,\n",
    "        lin_layer_dropouts,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        emb_dims: List of two element tuples\n",
    "        This list will contain a two element tuple for each\n",
    "        categorical feature. The first element of a tuple will\n",
    "        denote the number of unique values of the categorical\n",
    "        feature. The second element will denote the embedding\n",
    "        dimension to be used for that feature.\n",
    "        no_of_cont: Integer\n",
    "        The number of continuous features in the data.\n",
    "        lin_layer_sizes: List of integers.\n",
    "        The size of each linear layer. The length will be equal\n",
    "        to the total number\n",
    "        of linear layers in the network.\n",
    "        output_size: Integer\n",
    "        The size of the final output.\n",
    "        emb_dropout: Float\n",
    "        The dropout to be used after the embedding layers.\n",
    "        lin_layer_dropouts: List of floats\n",
    "        The dropouts to be used after each linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n",
    "\n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.no_of_cont = no_of_cont\n",
    "\n",
    "        # Linear Layers\n",
    "        first_lin_layer = nn.Linear(\n",
    "            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n",
    "        )\n",
    "\n",
    "        self.lin_layers = nn.ModuleList(\n",
    "            [first_lin_layer]\n",
    "            + [\n",
    "                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "                for i in range(len(lin_layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "        # Batch Norm Layers\n",
    "        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "        self.bn_layers = nn.ModuleList(\n",
    "            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n",
    "        )\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList(\n",
    "            [nn.Dropout(size) for size in lin_layer_dropouts]\n",
    "        )\n",
    "\n",
    "    def forward(self, cont_data, cat_data):\n",
    "        \n",
    "        if self.no_of_embs != 0:\n",
    "            x = [\n",
    "                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n",
    "            ]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout_layer(x)\n",
    "\n",
    "        if self.no_of_cont != 0:\n",
    "            normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "\n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, normalized_cont_data], 1)\n",
    "            else:\n",
    "                x = normalized_cont_data\n",
    "\n",
    "        for lin_layer, dropout_layer, bn_layer in zip(\n",
    "            self.lin_layers, self.droput_layers, self.bn_layers\n",
    "        ):\n",
    "\n",
    "            x = F.relu(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('data_feat.h5', 'table')\n",
    "\n",
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month', 'date_block_num'], axis=1).fillna(-1)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month', 'date_block_num'], axis=1).fillna(-1)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month', 'date_block_num'], axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.clip(0, 20)\n",
    "Y_valid = Y_valid.clip(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#unseen shop_id in valid: 5413\n",
      "#unseen shop_id in test: 5100\n",
      "#unseen item_id in valid: 21076\n",
      "#unseen item_id in test: 27174\n",
      "#unseen shop_category in valid: 0\n",
      "#unseen shop_category in test: 0\n",
      "#unseen shop_city in valid: 0\n",
      "#unseen shop_city in test: 0\n",
      "#unseen item_category_id in valid: 0\n",
      "#unseen item_category_id in test: 42\n",
      "#unseen name2 in valid: 176\n",
      "#unseen name2 in test: 924\n",
      "#unseen name3 in valid: 1232\n",
      "#unseen name3 in test: 2310\n",
      "#unseen subtype_code in valid: 0\n",
      "#unseen subtype_code in test: 42\n",
      "#unseen type_code in valid: 0\n",
      "#unseen type_code in test: 0\n",
      "#unseen month in valid: 0\n",
      "#unseen month in test: 0\n",
      "#unseen days in valid: 0\n",
      "#unseen days in test: 0\n"
     ]
    }
   ],
   "source": [
    "categorical_features = ['shop_id', 'item_id', 'shop_category', 'shop_city', 'item_category_id', 'name2', 'name3', 'subtype_code', 'type_code', 'month', 'days']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for cat_col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train[cat_col])\n",
    "    le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    unseen = max(le_dict.values()) + 1\n",
    "    X_train[cat_col] = X_train[cat_col].apply(lambda x: le_dict.get(x, unseen))\n",
    "    X_valid[cat_col] = X_valid[cat_col].apply(lambda x: le_dict.get(x, unseen))\n",
    "    X_test[cat_col] = X_test[cat_col].apply(lambda x: le_dict.get(x, unseen))\n",
    "    print(\"#unseen {} in valid: {}\".format(cat_col, sum(X_valid[cat_col] == unseen)))\n",
    "    print(\"#unseen {} in test: {}\".format(cat_col, sum(X_test[cat_col] == unseen)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(data=X_train, label=Y_train, cat_cols=categorical_features)\n",
    "valid_dataset = TabularDataset(data=X_valid, label=Y_valid, cat_cols=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=1)\n",
    "valid_loader = DataLoader(valid_dataset, batchsize, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 20041, 6, 32, 82, 157, 1530, 65, 11, 13, 4]\n",
      "[(55, 28), (20041, 50), (6, 3), (32, 16), (82, 41), (157, 50), (1530, 50), (65, 33), (11, 6), (13, 7), (4, 2)]\n"
     ]
    }
   ],
   "source": [
    "cat_dims = [int(X_train[col].nunique()) + 1 for col in categorical_features]\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "\n",
    "print(cat_dims)\n",
    "print(emb_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = FeedForwardNN(emb_dims, no_of_cont=21, lin_layer_sizes=[50, 100],\n",
    "                          output_size=1, emb_dropout=0.2,\n",
    "                          lin_layer_dropouts=[0.2,0.2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.124400 \tValidation Loss: 0.966090\n",
      "1018.3296625614166\n",
      "Validation loss decreased (inf --> 0.966090).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.045522 \tValidation Loss: 1.012167\n",
      "1009.1624648571014\n",
      "Epoch: 3 \tTraining Loss: 1.027238 \tValidation Loss: 0.994586\n",
      "1009.5293979644775\n",
      "Epoch: 4 \tTraining Loss: 1.016786 \tValidation Loss: 0.973187\n",
      "1006.6833546161652\n",
      "Epoch: 5 \tTraining Loss: 1.009292 \tValidation Loss: 0.992188\n",
      "1008.3474681377411\n",
      "Epoch: 6 \tTraining Loss: 1.005113 \tValidation Loss: 1.003348\n",
      "1008.9519309997559\n",
      "Epoch: 7 \tTraining Loss: 1.002839 \tValidation Loss: 0.995489\n",
      "1007.4971539974213\n",
      "Epoch: 8 \tTraining Loss: 1.000478 \tValidation Loss: 1.023904\n",
      "1016.1724345684052\n",
      "Epoch: 9 \tTraining Loss: 0.998852 \tValidation Loss: 0.944548\n",
      "1024.5817074775696\n",
      "Validation loss decreased (0.966090 --> 0.944548).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.995400 \tValidation Loss: 0.950251\n",
      "1032.1049454212189\n",
      "Epoch: 11 \tTraining Loss: 0.997604 \tValidation Loss: 0.956379\n",
      "1030.9826402664185\n",
      "Epoch: 12 \tTraining Loss: 0.993814 \tValidation Loss: 0.932309\n",
      "1016.6260223388672\n",
      "Validation loss decreased (0.944548 --> 0.932309).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.996430 \tValidation Loss: 0.978382\n",
      "1011.8423030376434\n",
      "Epoch: 14 \tTraining Loss: 0.993621 \tValidation Loss: 0.976593\n",
      "1007.6585986614227\n",
      "Epoch: 15 \tTraining Loss: 0.993230 \tValidation Loss: 1.021491\n",
      "1020.8821206092834\n",
      "Epoch: 16 \tTraining Loss: 0.993812 \tValidation Loss: 1.017020\n",
      "1014.8305587768555\n",
      "Epoch: 17 \tTraining Loss: 0.992971 \tValidation Loss: 1.025400\n",
      "1029.1185574531555\n",
      "Epoch: 18 \tTraining Loss: 0.997206 \tValidation Loss: 0.968681\n",
      "1024.7589328289032\n",
      "Epoch: 19 \tTraining Loss: 0.993765 \tValidation Loss: 0.995477\n",
      "1019.5753812789917\n",
      "Epoch: 20 \tTraining Loss: 0.992121 \tValidation Loss: 0.952906\n",
      "1029.8324139118195\n",
      "Epoch: 21 \tTraining Loss: 0.990151 \tValidation Loss: 0.979840\n",
      "1018.4105312824249\n",
      "Epoch: 22 \tTraining Loss: 0.993014 \tValidation Loss: 1.034660\n",
      "1023.811506986618\n",
      "Epoch: 23 \tTraining Loss: 0.992467 \tValidation Loss: 0.994991\n",
      "1016.3685653209686\n",
      "Epoch: 24 \tTraining Loss: 0.990610 \tValidation Loss: 0.938454\n",
      "1016.0350158214569\n",
      "Epoch: 25 \tTraining Loss: 0.991058 \tValidation Loss: 0.981919\n",
      "1028.3996543884277\n",
      "Epoch: 26 \tTraining Loss: 0.989071 \tValidation Loss: 0.947310\n",
      "1014.9613687992096\n",
      "Epoch: 27 \tTraining Loss: 0.991390 \tValidation Loss: 1.036332\n",
      "1007.4088652133942\n",
      "Epoch: 28 \tTraining Loss: 0.990306 \tValidation Loss: 0.992876\n",
      "1011.2116780281067\n",
      "Epoch: 29 \tTraining Loss: 0.989071 \tValidation Loss: 0.951195\n",
      "1007.9628143310547\n",
      "Epoch: 30 \tTraining Loss: 0.991706 \tValidation Loss: 0.995695\n",
      "1010.7143867015839\n",
      "Epoch: 31 \tTraining Loss: 0.990667 \tValidation Loss: 1.017291\n",
      "1008.6853775978088\n",
      "Epoch: 32 \tTraining Loss: 0.989847 \tValidation Loss: 1.020170\n",
      "1008.5147159099579\n",
      "Epoch: 33 \tTraining Loss: 0.989374 \tValidation Loss: 0.976302\n",
      "1013.3883707523346\n",
      "Epoch: 34 \tTraining Loss: 0.990178 \tValidation Loss: 0.993222\n",
      "1015.6540729999542\n",
      "Epoch: 35 \tTraining Loss: 0.991603 \tValidation Loss: 1.007510\n",
      "1019.6778934001923\n",
      "Epoch: 36 \tTraining Loss: 0.989557 \tValidation Loss: 1.003750\n",
      "1027.3703322410583\n",
      "Epoch: 37 \tTraining Loss: 0.991079 \tValidation Loss: 1.058877\n",
      "1023.9070885181427\n",
      "Epoch: 38 \tTraining Loss: 0.991909 \tValidation Loss: 1.014726\n",
      "1042.7443914413452\n",
      "Epoch: 39 \tTraining Loss: 0.989913 \tValidation Loss: 0.957270\n",
      "1068.0523426532745\n",
      "Epoch: 40 \tTraining Loss: 0.989353 \tValidation Loss: 0.995096\n",
      "1061.8670194149017\n",
      "Epoch: 41 \tTraining Loss: 0.989916 \tValidation Loss: 1.019555\n",
      "1043.7069599628448\n",
      "Epoch: 42 \tTraining Loss: 0.989836 \tValidation Loss: 0.992676\n",
      "1038.0953481197357\n",
      "Epoch: 43 \tTraining Loss: 0.988058 \tValidation Loss: 1.039283\n",
      "1023.45800614357\n",
      "Epoch: 44 \tTraining Loss: 0.988476 \tValidation Loss: 1.108643\n",
      "1054.2974200248718\n",
      "Epoch: 45 \tTraining Loss: 0.986974 \tValidation Loss: 1.040331\n",
      "1012.7823328971863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c99a381a8d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcat_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "no_of_epochs = 100\n",
    "\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "for epoch in range(no_of_epochs):\n",
    "    ts = time.time()\n",
    "    # monitor the training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ####\n",
    "    # train the model\n",
    "    ####\n",
    "    model.train()\n",
    "    for y, cont_x, cat_x in dataloader:\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y  = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(cont_x, cat_x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * cat_x.size(0)\n",
    "          \n",
    "    model.eval()\n",
    "    for y, cont_x, cat_x in valid_loader:\n",
    "        cat_x = cat_x.to(device)\n",
    "        cont_x = cont_x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = model(cont_x, cat_x)\n",
    "        output = torch.clamp(output, min=0, max=20)\n",
    "        loss = criterion(output, y)\n",
    "        valid_loss += loss.item() * cat_x.size(0)\n",
    "    \n",
    "    train_loss = math.sqrt(train_loss/len(dataloader.sampler))\n",
    "    valid_loss = math.sqrt(valid_loss/len(valid_loader.sampler))\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    print(time.time() - ts)\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pt', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TabularDataset(data=X_test, label=None, cat_cols=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=1)\n",
    "\n",
    "Y_test = []\n",
    "model.eval()\n",
    "\n",
    "for y, cont_x, cat_x in test_loader:\n",
    "    cat_x = cat_x.to(device)\n",
    "    cont_x = cont_x.to(device)\n",
    "    \n",
    "    output = model(cont_x, cat_x)\n",
    "    output = torch.clamp(output, min=0, max=20)\n",
    "    Y_test.append(output.data.cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.vstack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"ID\": np.arange(len(Y_test)), \n",
    "    \"item_cnt_month\": Y_test.flatten()\n",
    "})\n",
    "submission.to_csv('neural_networks_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
